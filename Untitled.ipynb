{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ced9b1a4-d572-4e93-8592-538d0ce5b0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Image Tagger with Color Detection\n",
      "==================================================\n",
      "Analyzing full body image: images/person5.jpg\n",
      "============================================================\n",
      "SKIN SHADES:\n",
      "   brown: 0.252 (25.2%)\n",
      "   red: 0.251 (25.1%)\n",
      "   fair: 0.251 (25.1%)\n",
      "üëï TOPWEAR:\n",
      "   dress shirt: 0.061 (6.1%)\n",
      "   polo shirt: 0.060 (6.0%)\n",
      "   vest: 0.060 (6.0%)\n",
      "\n",
      "üëñ BOTTOMWEAR:\n",
      "   dress pants: 0.068 (6.8%)\n",
      "   chinos: 0.068 (6.8%)\n",
      "   trousers: 0.067 (6.7%)\n",
      "\n",
      "üëó FULL OUTFIT CHECK:\n",
      "   overall: 0.104 (10.4%)\n",
      "\n",
      "üé® COLORS:\n",
      "   Topwear (dress shirt):\n",
      "     navy: 0.048 (4.8%)\n",
      "     khaki: 0.048 (4.8%)\n",
      "     brown: 0.048 (4.8%)\n",
      "   Bottomwear (dress pants):\n",
      "     khaki: 0.049 (4.9%)\n",
      "     brown: 0.048 (4.8%)\n",
      "     navy: 0.048 (4.8%)\n",
      "\n",
      "üîç ALTERNATIVE COLOR CHECK:\n",
      "   Alternative top color: khaki (4.9%)\n",
      "   Alternative bottom color: khaki (4.9%)\n",
      "\n",
      "üìù FINAL OUTFIT DESCRIPTION:\n",
      "   navy dress shirt and khaki dress pants\n",
      "\n",
      "‚ú® Final Result: navy dress shirt and khaki dress pants\n"
     ]
    }
   ],
   "source": [
    "# Simple CLIP Image Tagger - Just put in an image path and get tags!\n",
    "\n",
    "# Install required packages first:\n",
    "# !pip install torch torchvision clip-by-openai pillow\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def tag_image(image_path):\n",
    "    \"\"\"\n",
    "    Analyze full body images to detect topwear, bottomwear, and their colors\n",
    "    Just provide the path to your image!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Topwear categories\n",
    "    topwear = [\n",
    "        \"t-shirt\", \"shirt\", \"blouse\", \"tank top\", \"polo shirt\", \"dress shirt\",\n",
    "        \"sweater\", \"hoodie\", \"jacket\", \"blazer\", \"coat\", \"cardigan\", \n",
    "        \"vest\", \"crop top\", \"tube top\", \"halter top\", \"camisole\"\n",
    "    ]\n",
    "    \n",
    "    # Bottomwear categories  \n",
    "    bottomwear = [\n",
    "        \"jeans\", \"pants\", \"trousers\", \"shorts\", \"skirt\", \"leggings\",\n",
    "        \"sweatpants\", \"chinos\", \"cargo pants\", \"dress pants\", \"joggers\",\n",
    "        \"capris\", \"culottes\", \"palazzo pants\", \"wide leg pants\"\n",
    "    ]\n",
    "    \n",
    "    # Dresses and full outfits\n",
    "    full_outfits = [\n",
    "        \"dress\", \"gown\", \"sundress\", \"maxi dress\", \"mini dress\", \n",
    "        \"cocktail dress\", \"evening dress\", \"jumpsuit\", \"romper\", \"overall\"\n",
    "    ]\n",
    "    \n",
    "    # Colors to test against\n",
    "    colors = [\n",
    "        \"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\", \"pink\",\n",
    "        \"black\", \"white\", \"gray\", \"brown\", \"navy\", \"beige\", \"khaki\",\n",
    "        \"maroon\", \"teal\", \"olive\", \"cream\", \"gold\", \"silver\", \"denim blue\"\n",
    "    ]\n",
    "\n",
    "    skin_shades = [\n",
    "    \"fair\", \"brown\", \"ashy\", \"red\"\n",
    "]\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        print(f\"Analyzing full body image: {image_path}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # ==== SKIN COLOR DETECTION =====\n",
    "        skin_prompt = [f\"person skin color is {colors}\" for colors in skin_shades]\n",
    "        skin_inputs = clip.tokenize(skin_prompt).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input)\n",
    "            skin_features = model.encode_text(skin_inputs)\n",
    "            \n",
    "            # Normalize features\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            skin_features = skin_features / skin_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "            # Calculate similarities\n",
    "            skin_similarities = (image_features @ skin_features.T).softmax(dim=-1)\n",
    "            \n",
    "            # Get top 3 topwear predictions\n",
    "            skin_probs, skin_indices = skin_similarities[0].topk(3)\n",
    "            \n",
    "            print(\"SKIN SHADES:\")\n",
    "            skin_shade_results = []\n",
    "            for i in range(3):\n",
    "                item = skin_shades[skin_indices[i]]\n",
    "                confidence = skin_probs[i].item()\n",
    "                print(f\"   {item}: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
    "                skin_shade_results.append((item, confidence))\n",
    "        \n",
    "        # ===== TOPWEAR DETECTION =====\n",
    "        top_prompts = [f\"person wearing {item}\" for item in topwear]\n",
    "        top_inputs = clip.tokenize(top_prompts).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input)\n",
    "            top_features = model.encode_text(top_inputs)\n",
    "            \n",
    "            # Normalize features\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            top_features = top_features / top_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            top_similarities = (image_features @ top_features.T).softmax(dim=-1)\n",
    "            \n",
    "            # Get top 3 topwear predictions\n",
    "            top_probs, top_indices = top_similarities[0].topk(3)\n",
    "            \n",
    "            print(\"üëï TOPWEAR:\")\n",
    "            topwear_results = []\n",
    "            for i in range(3):\n",
    "                item = topwear[top_indices[i]]\n",
    "                confidence = top_probs[i].item()\n",
    "                print(f\"   {item}: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
    "                topwear_results.append((item, confidence))\n",
    "        \n",
    "        # ===== BOTTOMWEAR DETECTION =====\n",
    "        bottom_prompts = [f\"person wearing {item}\" for item in bottomwear]\n",
    "        bottom_inputs = clip.tokenize(bottom_prompts).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            bottom_features = model.encode_text(bottom_inputs)\n",
    "            bottom_features = bottom_features / bottom_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            bottom_similarities = (image_features @ bottom_features.T).softmax(dim=-1)\n",
    "            \n",
    "            # Get top 3 bottomwear predictions\n",
    "            bottom_probs, bottom_indices = bottom_similarities[0].topk(3)\n",
    "            \n",
    "            print(\"\\nüëñ BOTTOMWEAR:\")\n",
    "            bottomwear_results = []\n",
    "            for i in range(3):\n",
    "                item = bottomwear[bottom_indices[i]]\n",
    "                confidence = bottom_probs[i].item()\n",
    "                print(f\"   {item}: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
    "                bottomwear_results.append((item, confidence))\n",
    "        \n",
    "        # ===== FULL OUTFIT CHECK =====\n",
    "        outfit_prompts = [f\"person wearing {item}\" for item in full_outfits]\n",
    "        outfit_inputs = clip.tokenize(outfit_prompts).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outfit_features = model.encode_text(outfit_inputs)\n",
    "            outfit_features = outfit_features / outfit_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Calculate similarities\n",
    "            outfit_similarities = (image_features @ outfit_features.T).softmax(dim=-1)\n",
    "            \n",
    "            # Get top outfit prediction\n",
    "            outfit_prob, outfit_index = outfit_similarities[0].topk(1)\n",
    "            \n",
    "            best_outfit = full_outfits[outfit_index[0]]\n",
    "            outfit_confidence = outfit_prob[0].item()\n",
    "            \n",
    "            print(f\"\\nüëó FULL OUTFIT CHECK:\")\n",
    "            print(f\"   {best_outfit}: {outfit_confidence:.3f} ({outfit_confidence*100:.1f}%)\")\n",
    "        \n",
    "        # ===== IMPROVED COLOR DETECTION =====\n",
    "        print(f\"\\nüé® COLORS:\")\n",
    "        \n",
    "        # Method 1: Direct color detection for specific clothing items\n",
    "        best_top = topwear_results[0][0]\n",
    "        best_bottom = bottomwear_results[0][0]\n",
    "        \n",
    "        # Top color detection with specific clothing item\n",
    "        top_color_prompts = [f\"person wearing {color} {best_top}\" for color in colors]\n",
    "        top_color_inputs = clip.tokenize(top_color_prompts).to(device)\n",
    "        \n",
    "        # Bottom color detection with specific clothing item\n",
    "        bottom_color_prompts = [f\"person wearing {color} {best_bottom}\" for color in colors]\n",
    "        bottom_color_inputs = clip.tokenize(bottom_color_prompts).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            top_color_features = model.encode_text(top_color_inputs)\n",
    "            bottom_color_features = model.encode_text(bottom_color_inputs)\n",
    "            \n",
    "            top_color_features = top_color_features / top_color_features.norm(dim=-1, keepdim=True)\n",
    "            bottom_color_features = bottom_color_features / bottom_color_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Calculate color similarities\n",
    "            top_color_sims = (image_features @ top_color_features.T).softmax(dim=-1)\n",
    "            bottom_color_sims = (image_features @ bottom_color_features.T).softmax(dim=-1)\n",
    "            \n",
    "            # Get top 3 colors for each\n",
    "            top_color_probs, top_color_indices = top_color_sims[0].topk(3)\n",
    "            bottom_color_probs, bottom_color_indices = bottom_color_sims[0].topk(3)\n",
    "            \n",
    "            print(f\"   Topwear ({best_top}):\")\n",
    "            top_color_results = []\n",
    "            for i in range(3):\n",
    "                color = colors[top_color_indices[i]]\n",
    "                confidence = top_color_probs[i].item()\n",
    "                print(f\"     {color}: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
    "                top_color_results.append((color, confidence))\n",
    "                \n",
    "            print(f\"   Bottomwear ({best_bottom}):\")\n",
    "            bottom_color_results = []\n",
    "            for i in range(3):\n",
    "                color = colors[bottom_color_indices[i]]\n",
    "                confidence = bottom_color_probs[i].item()\n",
    "                print(f\"     {color}: {confidence:.3f} ({confidence*100:.1f}%)\")\n",
    "                bottom_color_results.append((color, confidence))\n",
    "        \n",
    "        # Method 2: Alternative detection with different prompts\n",
    "        print(f\"\\nüîç ALTERNATIVE COLOR CHECK:\")\n",
    "        \n",
    "        # Try with \"wearing\" vs \"has\" prompts\n",
    "        alt_top_prompts = [f\"{color} {best_top}\" for color in colors]\n",
    "        alt_bottom_prompts = [f\"{color} {best_bottom}\" for color in colors]\n",
    "        \n",
    "        alt_top_inputs = clip.tokenize(alt_top_prompts).to(device)\n",
    "        alt_bottom_inputs = clip.tokenize(alt_bottom_prompts).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            alt_top_features = model.encode_text(alt_top_inputs)\n",
    "            alt_bottom_features = model.encode_text(alt_bottom_inputs)\n",
    "            \n",
    "            alt_top_features = alt_top_features / alt_top_features.norm(dim=-1, keepdim=True)\n",
    "            alt_bottom_features = alt_bottom_features / alt_bottom_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            alt_top_sims = (image_features @ alt_top_features.T).softmax(dim=-1)\n",
    "            alt_bottom_sims = (image_features @ alt_bottom_features.T).softmax(dim=-1)\n",
    "            \n",
    "            alt_top_prob, alt_top_idx = alt_top_sims[0].topk(1)\n",
    "            alt_bottom_prob, alt_bottom_idx = alt_bottom_sims[0].topk(1)\n",
    "            \n",
    "            alt_top_color = colors[alt_top_idx[0]]\n",
    "            alt_bottom_color = colors[alt_bottom_idx[0]]\n",
    "            \n",
    "            print(f\"   Alternative top color: {alt_top_color} ({alt_top_prob[0].item()*100:.1f}%)\")\n",
    "            print(f\"   Alternative bottom color: {alt_bottom_color} ({alt_bottom_prob[0].item()*100:.1f}%)\")\n",
    "        \n",
    "        # Choose best colors (highest confidence)\n",
    "        final_top_color = top_color_results[0][0]\n",
    "        final_bottom_color = bottom_color_results[0][0]\n",
    "        \n",
    "        # Use alternative if much more confident\n",
    "        if alt_top_prob[0].item() > top_color_results[0][1] + 0.1:\n",
    "            final_top_color = alt_top_color\n",
    "        if alt_bottom_prob[0].item() > bottom_color_results[0][1] + 0.1:\n",
    "            final_bottom_color = alt_bottom_color\n",
    "        \n",
    "        # ===== FINAL DESCRIPTION =====\n",
    "        best_top = topwear_results[0][0]\n",
    "        best_bottom = bottomwear_results[0][0]\n",
    "        \n",
    "        # Check if it's more likely a dress/full outfit\n",
    "        if outfit_confidence > 0.3:  # If dress/outfit confidence is high\n",
    "            # For dresses, use the top color detection\n",
    "            dress_color = final_top_color\n",
    "            description = f\"{dress_color} {best_outfit}\"\n",
    "            outfit_type = \"full_outfit\"\n",
    "        else:\n",
    "            description = f\"{final_top_color} {best_top} and {final_bottom_color} {best_bottom}\"\n",
    "            outfit_type = \"separates\"\n",
    "        \n",
    "        print(f\"\\nüìù FINAL OUTFIT DESCRIPTION:\")\n",
    "        print(f\"   {description}\")\n",
    "        \n",
    "        return {\n",
    "            'topwear': topwear_results,\n",
    "            'bottomwear': bottomwear_results,\n",
    "            'full_outfit': (best_outfit, outfit_confidence),\n",
    "            'top_color': final_top_color,\n",
    "            'bottom_color': final_bottom_color,\n",
    "            'top_color_options': top_color_results,\n",
    "            'bottom_color_options': bottom_color_results,\n",
    "            'description': description,\n",
    "            'type': outfit_type\n",
    "        }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your image path\n",
    "    image_path = \"images/person5.jpg\"  # Change this to your actual image path\n",
    "    \n",
    "    print(\"CLIP Image Tagger with Color Detection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Tag the image\n",
    "    results = tag_image(image_path)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n‚ú® Final Result: {results['description']}\")\n",
    "\n",
    "# You can also create quick functions:\n",
    "def quick_outfit_description(image_path):\n",
    "    \"\"\"One-liner to get full outfit description\"\"\"\n",
    "    result = tag_image(image_path)\n",
    "    return result['description'] if result else \"Could not analyze image\"\n",
    "\n",
    "def get_outfit_breakdown(image_path):\n",
    "    \"\"\"Get detailed breakdown of the outfit\"\"\"\n",
    "    result = tag_image(image_path)\n",
    "    if result:\n",
    "        if result['type'] == 'full_outfit':\n",
    "            return f\"Wearing a {result['description']}\"\n",
    "        else:\n",
    "            return f\"Top: {result['top_color']} {result['topwear'][0][0]}, Bottom: {result['bottom_color']} {result['bottomwear'][0][0]}\"\n",
    "    return \"Could not analyze image\"\n",
    "\n",
    "# Usage examples:\n",
    "# quick_outfit_description(\"my_outfit.jpg\") \n",
    "# # Returns: \"black t-shirt and blue jeans\" or \"red dress\"\n",
    "\n",
    "# get_outfit_breakdown(\"my_outfit.jpg\")\n",
    "# # Returns: \"Top: black t-shirt, Bottom: blue jeans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df817e-e93d-437d-83e4-50cd7e1f67a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
